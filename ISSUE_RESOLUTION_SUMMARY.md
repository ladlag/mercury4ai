# é—®é¢˜è§£å†³æ€»ç»“ / Issue Resolution Summary

## åŸå§‹é—®é¢˜ / Original Issue

**ç”¨æˆ·åé¦ˆï¼š**
> "åˆ°åº•æœ‰æ²¡æœ‰llmæ•°æ®æ¸…æ´—åŠŸèƒ½å•Šï¼Œæ€ä¹ˆæ”¹äº†è¿™ä¹ˆå¤šééƒ½æ²¡æœ‰çœ‹åˆ°æ‰§è¡Œã€ä¹Ÿæ²¡æœ‰æ—¥å¿—ã€æ²¡æœ‰æ–‡ä»¶ç”Ÿæˆï¼Ÿ"

**ç¿»è¯‘ï¼š**
> "Does it really have LLM data cleaning functionality? Why after so many modifications, I still don't see execution, no logs, and no file generation?"

---

## é—®é¢˜åˆ†æ / Problem Analysis

### æ ¹æœ¬åŸå›  / Root Causes

1. **æ—¥å¿—çº§åˆ«é—®é¢˜**: å…³é”®çš„æ¸…æ´—æ—¥å¿—ä½¿ç”¨ `DEBUG` çº§åˆ«ï¼Œç”¨æˆ·åœ¨æ­£å¸¸æ¨¡å¼ä¸‹çœ‹ä¸åˆ°
   - **Log level issue**: Critical cleaning logs used `DEBUG` level, invisible to users in normal mode

2. **æœ¯è¯­ä¸æ¸…æ™°**: æ—¥å¿—ä½¿ç”¨æŠ€æœ¯æœ¯è¯­ï¼ˆå¦‚ "fit_markdown", "PruningContentFilter"ï¼‰è€Œéç”¨æˆ·å‹å¥½çš„è¯­è¨€
   - **Unclear terminology**: Logs used technical terms instead of user-friendly language

3. **ç¼ºå°‘æ¦‚è§ˆ**: ä»»åŠ¡å¼€å§‹æ—¶æ²¡æœ‰æ¸…æ¥šæ˜¾ç¤ºå¯ç”¨äº†å“ªäº›æ¸…æ´—é˜¶æ®µ
   - **Missing overview**: No clear indication at task start of which cleaning stages are enabled

4. **ç¼ºå°‘æ€»ç»“**: ä»»åŠ¡ç»“æŸæ—¶æ²¡æœ‰æ€»ç»“ç”Ÿæˆäº†å“ªäº›æ–‡ä»¶
   - **Missing summary**: No summary of what files were generated at task completion

---

## è§£å†³æ–¹æ¡ˆ / Solution

### æ ¸å¿ƒæ”¹è¿› / Core Improvements

âœ… **1. æå‡æ—¥å¿—å¯è§æ€§**
- å°†å…³é”®æ—¥å¿—ä» DEBUG æå‡åˆ° INFO çº§åˆ«
- ç”¨æˆ·æ— éœ€å¯ç”¨è°ƒè¯•æ¨¡å¼å³å¯çœ‹åˆ°æ‰€æœ‰é‡è¦ä¿¡æ¯

âœ… **2. æ¸…æ™°çš„é˜¶æ®µæ ‡è¯†**
- æ‰€æœ‰æ—¥å¿—ä½¿ç”¨ "Stage 1" å’Œ "Stage 2" æ ‡è¯†
- Stage 1: crawl4ai è‡ªåŠ¨æ¸…æ´—ï¼ˆç§»é™¤ headers, footers, navigationï¼‰
- Stage 2: LLM ç»“æ„åŒ–æå–ï¼ˆä½¿ç”¨è‡ªå®šä¹‰ prompt å’Œ schemaï¼‰

âœ… **3. å¯åŠ¨é…ç½®æ¨ªå¹…**
- ä»»åŠ¡å¼€å§‹æ—¶æ˜¾ç¤ºå®Œæ•´çš„æ¸…æ´—é…ç½®
- æ˜ç¡®å‘ŠçŸ¥å“ªäº›åŠŸèƒ½å·²å¯ç”¨ã€å“ªäº›æœªå¯ç”¨

âœ… **4. å®æ—¶è¿›åº¦åé¦ˆ**
- æ¯ä¸ª URL å¤„ç†åæ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶ç±»å‹
- å®æ—¶çœ‹åˆ°æ¸…æ´—æ•ˆæœï¼ˆå­—ç¬¦æ•°å‡å°‘ç™¾åˆ†æ¯”ï¼‰

âœ… **5. å®Œæ•´æ‰§è¡Œæ€»ç»“**
- ä»»åŠ¡ç»“æŸæ—¶æ˜¾ç¤ºè¯¦ç»†çš„æ‰§è¡Œç»“æœ
- åŒ…æ‹¬æ¸…æ´—é˜¶æ®µã€ç”Ÿæˆçš„æ–‡ä»¶ã€MinIO å­˜å‚¨è·¯å¾„

---

## æ”¹è¿›åçš„æ—¥å¿—ç¤ºä¾‹ / Improved Log Examples

### åœºæ™¯ 1: åªæœ‰ Stage 1 æ¸…æ´—ï¼ˆæ—  LLMï¼‰

```
================================================================================
Starting crawl task: Simple Web Scraping
Task ID: abc123...
Run ID: def456...
URLs to crawl: 1
================================================================================
Data Cleaning Configuration:
  â€¢ Stage 1 (crawl4ai): ENABLED - Removes headers, footers, navigation
  â€¢ Stage 2 (LLM extraction): DISABLED - No prompt_template configured
    To enable Stage 2 extraction, add 'prompt_template' to task config
================================================================================
Stage 1 cleaning enabled: PruningContentFilter will remove headers, footers, and navigation
Executing crawl for: https://example.com
Extracted raw markdown: 6500 characters
Stage 1 cleaning completed: 6500 -> 3200 chars (reduced 50.8%)
Saved raw markdown to MinIO: 2026-01-08/def456/markdown/doc123.md
Saved cleaned markdown (Stage 1) to MinIO: 2026-01-08/def456/markdown/doc123_cleaned.md
âœ“ Successfully processed URL 1/1: https://example.com
  Generated files: raw markdown, cleaned markdown (Stage 1)
================================================================================
âœ“ Crawl task abc123 completed successfully
Summary:
  - URLs crawled: 1
  - URLs failed: 0
  - Documents created: 1
  - MinIO path: 2026-01-08/def456
  - Data cleaning performed: Stage 1 (crawl4ai cleaning)
================================================================================
```

### åœºæ™¯ 2: å®Œæ•´çš„ä¸¤é˜¶æ®µæ¸…æ´—ï¼ˆStage 1 + Stage 2ï¼‰

```
================================================================================
Starting crawl task: News Article Extraction
Task ID: abc123...
Run ID: def456...
URLs to crawl: 3
================================================================================
Data Cleaning Configuration:
  â€¢ Stage 1 (crawl4ai): ENABLED - Removes headers, footers, navigation
  â€¢ Stage 2 (LLM extraction): ENABLED - Extracts structured data
    - Provider: openai
    - Model: deepseek-chat
    - Prompt template: 234 characters
    - Output schema: configured
================================================================================
Processing URL 1/3: https://news.example.com/article1
Stage 1 cleaning enabled: PruningContentFilter will remove headers, footers, and navigation
Stage 2 extraction enabled: LLM will extract structured data using custom schema
Executing crawl for: https://news.example.com/article1
Extracted raw markdown: 8500 characters
Stage 1 cleaning completed: 8500 -> 3400 chars (reduced 60.0%)
Stage 2 extraction completed: Successfully extracted structured data from https://news.example.com/article1
Saved raw markdown to MinIO: 2026-01-08/def456/markdown/doc123.md
Saved cleaned markdown (Stage 1) to MinIO: 2026-01-08/def456/markdown/doc123_cleaned.md
Saved structured data (Stage 2) to MinIO: 2026-01-08/def456/json/doc123.json
âœ“ Successfully processed URL 1/3: https://news.example.com/article1
  Generated files: raw markdown, cleaned markdown (Stage 1), structured JSON (Stage 2)

[... å¤„ç†å…¶ä»– URL ...]

================================================================================
âœ“ Crawl task abc123 completed successfully
Summary:
  - URLs crawled: 3
  - URLs failed: 0
  - Documents created: 3
  - MinIO path: 2026-01-08/def456
  - Data cleaning performed: Stage 1 (crawl4ai cleaning), Stage 2 (LLM extraction)
================================================================================
```

---

## å¦‚ä½•æŸ¥çœ‹æ—¥å¿— / How to View Logs

### æ–¹æ³• 1: Docker Composeï¼ˆæ¨èï¼‰

```bash
# å®æ—¶æŸ¥çœ‹ worker æ—¥å¿—
docker compose logs -f worker

# æŸ¥çœ‹æœ€è¿‘ 100 è¡Œæ—¥å¿—
docker compose logs --tail=100 worker

# æŸ¥çœ‹æœ€è¿‘ 5 åˆ†é’Ÿçš„æ—¥å¿—
docker compose logs --since 5m worker
```

### æ–¹æ³• 2: é€šè¿‡ API

```bash
# è·å–è¿è¡Œæ—¥å¿—å’Œ manifest
curl -H "X-API-Key: your-api-key" \
  "http://localhost:8000/api/runs/{run_id}/logs"
```

---

## ç”Ÿæˆçš„æ–‡ä»¶ä½ç½® / Generated Files Location

æ‰€æœ‰æ–‡ä»¶å­˜å‚¨åœ¨ MinIO ä¸­ï¼ŒæŒ‰ç…§ä»¥ä¸‹ç»“æ„ç»„ç»‡ï¼š

```
mercury4ai/
â””â”€â”€ {YYYY-MM-DD}/                    # æ—¥æœŸ / Date
    â””â”€â”€ {run_id}/                    # è¿è¡Œ ID / Run ID
        â”œâ”€â”€ markdown/
        â”‚   â”œâ”€â”€ {doc_id}.md                  # åŸå§‹ markdown / Raw markdown
        â”‚   â””â”€â”€ {doc_id}_cleaned.md          # æ¸…æ´—å markdown / Cleaned markdown (Stage 1)
        â”œâ”€â”€ json/
        â”‚   â””â”€â”€ {doc_id}.json                # ç»“æ„åŒ–æ•°æ® / Structured data (Stage 2)
        â”œâ”€â”€ images/
        â”‚   â””â”€â”€ {filename}                   # å›¾ç‰‡ / Images
        â””â”€â”€ logs/
            â”œâ”€â”€ run_manifest.json            # è¿è¡Œæ¸…å• / Run manifest
            â””â”€â”€ resource_index.json          # èµ„æºç´¢å¼• / Resource index
```

---

## éªŒè¯æ”¹è¿›æ˜¯å¦ç”Ÿæ•ˆ / Verify Improvements

### å¿«é€ŸéªŒè¯æ­¥éª¤

1. **å¯åŠ¨æœåŠ¡**
   ```bash
   docker compose up -d
   ```

2. **åˆ›å»ºæµ‹è¯•ä»»åŠ¡**ï¼ˆä½¿ç”¨ç¤ºä¾‹é…ç½®ï¼‰
   ```bash
   curl -X POST "http://localhost:8000/api/tasks/import?format=yaml" \
     -H "X-API-Key: your-api-key" \
     -H "Content-Type: text/plain" \
     --data-binary @examples/task_chinese_llm_deepseek.json
   ```

3. **è¿è¡Œä»»åŠ¡**
   ```bash
   curl -X POST "http://localhost:8000/api/tasks/{task_id}/run" \
     -H "X-API-Key: your-api-key"
   ```

4. **æŸ¥çœ‹æ—¥å¿—**
   ```bash
   docker compose logs -f worker
   ```

### é¢„æœŸçœ‹åˆ°çš„å†…å®¹

âœ“ å¯åŠ¨æ¨ªå¹…æ˜¾ç¤ºæ¸…æ´—é…ç½®
âœ“ "Stage 1 cleaning enabled" æ¶ˆæ¯
âœ“ "Stage 1 cleaning completed" å¸¦æœ‰ç»Ÿè®¡æ•°æ®
âœ“ "Stage 2 extraction enabled/disabled" æ¶ˆæ¯
âœ“ æ¯ä¸ª URL çš„å¤„ç†æ€»ç»“
âœ“ æœ€ç»ˆä»»åŠ¡å®Œæˆæ€»ç»“

---

## å¸¸è§é—®é¢˜è§£ç­” / FAQ

### Q1: ä¸ºä»€ä¹ˆæˆ‘åªçœ‹åˆ° Stage 1ï¼Œæ²¡æœ‰ Stage 2ï¼Ÿ

**A:** Stage 2ï¼ˆLLM æå–ï¼‰éœ€è¦åœ¨ä»»åŠ¡é…ç½®ä¸­æ·»åŠ  `prompt_template`ã€‚

**ç¤ºä¾‹é…ç½®ï¼š**
```yaml
prompt_template: |
  è¯·ä»æ–‡ç« ä¸­æå–æ ‡é¢˜ã€å†…å®¹å’Œä½œè€…ä¿¡æ¯
output_schema:
  type: object
  properties:
    title: {type: string}
    content: {type: string}
    author: {type: string}
```

å‚è€ƒæ–‡æ¡£ï¼š`TROUBLESHOOTING_LLM_EXTRACTION.md`

### Q2: æ¸…æ´—ç‡æ˜¾ç¤º 0%ï¼Œæ˜¯ä¸æ˜¯æ²¡æœ‰å·¥ä½œï¼Ÿ

**A:** å¯èƒ½åŸå› ï¼š
1. é¡µé¢æœ¬èº«å°±å¾ˆå¹²å‡€ï¼Œæ²¡æœ‰å†—ä½™çš„ headers/footers
2. ä½¿ç”¨ CSS é€‰æ‹©å™¨å¯ä»¥è·å¾—æ›´ç²¾ç¡®çš„æ¸…æ´—æ•ˆæœ

**è§£å†³æ–¹æ¡ˆï¼š**
```yaml
crawl_config:
  css_selector: "article, .main-content, .article-body"
```

### Q3: å¦‚ä½•ä¸‹è½½ç”Ÿæˆçš„æ–‡ä»¶ï¼Ÿ

**A:** é€šè¿‡ API è·å–é¢„ç­¾å URLï¼š
```bash
curl -H "X-API-Key: your-api-key" \
  "http://localhost:8000/api/runs/{run_id}/logs"
```

è¿”å›çš„ JSON ä¸­åŒ…å«æ‰€æœ‰æ–‡ä»¶çš„ä¸‹è½½é“¾æ¥ã€‚

---

## æŠ€æœ¯ç»†èŠ‚ / Technical Details

### ä¿®æ”¹çš„æ–‡ä»¶

1. **app/services/crawler_service.py**
   - æ—¥å¿—çº§åˆ«ä» DEBUG æå‡åˆ° INFO
   - æ·»åŠ æ¸…æ´—ç»Ÿè®¡ä¿¡æ¯
   - æ¸…æ™°çš„é˜¶æ®µæ ‡è¯†

2. **app/workers/crawl_worker.py**
   - å¯åŠ¨æ¨ªå¹…
   - æ¯ä¸ª URL çš„æ€»ç»“
   - æœ€ç»ˆæ‰§è¡Œæ€»ç»“

3. **VISIBILITY_IMPROVEMENTS.md**
   - å®Œæ•´çš„æ”¹è¿›è¯´æ˜æ–‡æ¡£
   - ç¤ºä¾‹å’Œæ•…éšœæ’é™¤æŒ‡å—

### å‘åå…¼å®¹æ€§

âœ… **å®Œå…¨å‘åå…¼å®¹**
- æ²¡æœ‰ API å˜æ›´
- æ²¡æœ‰é…ç½®æ ¼å¼å˜æ›´
- ç°æœ‰ä»»åŠ¡æ— éœ€ä¿®æ”¹
- ä»…æ—¥å¿—è¾“å‡ºæ ¼å¼å¾—åˆ°æ”¹è¿›

---

## ç›¸å…³æ–‡æ¡£ / Related Documentation

ğŸ“š **å®Œæ•´æ–‡æ¡£é›†ï¼š**
- [VISIBILITY_IMPROVEMENTS.md](VISIBILITY_IMPROVEMENTS.md) - æœ¬æ¬¡æ”¹è¿›çš„è¯¦ç»†è¯´æ˜
- [TROUBLESHOOTING_LLM_EXTRACTION.md](TROUBLESHOOTING_LLM_EXTRACTION.md) - LLM æå–é—®é¢˜æ’æŸ¥
- [CONFIG.md](CONFIG.md) - å®Œæ•´é…ç½®æŒ‡å—
- [README.md](README.md) - é¡¹ç›®æ¦‚è§ˆå’Œå¿«é€Ÿå¼€å§‹

---

## æ€»ç»“ / Summary

âœ… **é—®é¢˜å·²è§£å†³ / Issue Resolved**

é€šè¿‡è¿™æ¬¡æ”¹è¿›ï¼Œç³»ç»Ÿç°åœ¨èƒ½å¤Ÿï¼š

1. âœ“ æ¸…æ¥šåœ°æ˜¾ç¤ºæ•°æ®æ¸…æ´—åŠŸèƒ½æ­£åœ¨æ‰§è¡Œ
2. âœ“ å®æ—¶æ˜¾ç¤ºæ¸…æ´—è¿›åº¦å’Œæ•ˆæœ
3. âœ“ æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ç”Ÿæˆäº†å“ªäº›æ–‡ä»¶
4. âœ“ å¸®åŠ©ç”¨æˆ·å¿«é€Ÿè¯Šæ–­é…ç½®é—®é¢˜

ç”¨æˆ·ä¸å†éœ€è¦çŒœæµ‹åŠŸèƒ½æ˜¯å¦åœ¨å·¥ä½œï¼Œæ‰€æœ‰ä¿¡æ¯éƒ½æ¸…æ™°å¯è§ï¼

---

**å˜æ›´ä½œè€… / Change Author**: GitHub Copilot  
**æ—¥æœŸ / Date**: 2026-01-08  
**çŠ¶æ€ / Status**: âœ“ å®Œæˆ / Complete
